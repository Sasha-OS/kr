1.Методи квазіньютона (QNM), як правило, є класом методів оптимізації, які використовуються в нелінійному програмуванні, коли повні методи Ньютона або забирають занадто багато часу, або їх важко використовувати. Більш конкретно, ці методи використовуються для знаходження глобального мінімуму функції f(x), яка двічі диференційована. Використання методів квазіньютона перед повним методом Ньютона має явні переваги для розширених і складних нелінійних задач. Однак ці методи не є досконалими і можуть мати деякі недоліки залежно від конкретного типу використовуваного методу квазіньютона та проблеми, до якої він застосовується. Незважаючи на це, квазіньютоновські методи, як правило, варто використовувати, за винятком дуже простих задач.

Метод квазіньютона, в порівнянні з Методом Ньоютона:
-Комп’ютерно дешевий
-Швидке обчислення
-Друга похідна не потрібна
-Немає необхідності розв’язувати лінійну систему рівнянь
-Більше кроків конвергенції
-Менш точний шлях конвергенції

Хоча останні два рядки , здається, є недоліками квазіньютоновського методу, більш швидкий час обчислення може в кінцевому підсумку збалансувати їх. Для великих і складних задач цей баланс є перевагою методів квазіньютона над повним методом Ньютона через загальний швидший час вирішення.


Я використовував архітектуру DeepMind Deep Q-Network (DQN), як апроксиматор функції для Q(s, a; w). Необроблені кадри Atari, які представляють собою зображення розміром 210 × 160 пікселів із 128 кольоровою палітрою, були попередньо оброблені шляхом перетворення їх відображення RGB у шкалу сірого, а потім зменшення вибірки до 110 × 84 пікселів. Остаточне вхідне представлення отримується шляхом обрізання ділянки зображення розміром 84 × 84, яка приблизно охоплює ігрову область. Стек останніх 4 послідовних кадрів використовувався для створення вхідних даних розміром (4×84×84) для Q-функції. Перший прихований шар мережі складався з 32 згорткових фільтрів розміром 8×8 з кроком 4, за яким слідував випрямлений лінійний блок (ReLU) для нелінійності. Другий прихований шар складався з 64 згорткових фільтрів розміром 4 × 4 з кроком 2, за яким слідувала функція ReLU. Третій шар складався з 512 повністю з'єднаних лінійних блоків, за якими слідував ReLU. Вихідний рівень був повністю зв’язаним лінійним шаром з виходом Q(s, ai, w) для кожної дійсної дії джойстика ai ∈ A. Кількість дійсних дій джойстика, тобто |A|, становила 6 для Space-Invaders(Моя гра).
Через кожні 10 000 кроків продуктивність алгоритму навчання перевірялася шляхом заморожування параметрів Q-мережі. Під час тестування я використовував ε = 0,05. Жадібну дію, maxa Q(s, a; w) вибирала Q-мережа в 95% випадків і мала 5% випадковості.